{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextSimilarityFinal.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxZutMtoJiQ_",
        "colab_type": "code",
        "outputId": "c4c9e940-5e1a-4d8d-b020-03c0c4179463",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd \"/content/drive/My Drive/Colab Notebooks\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieN3w1K1SpnQ",
        "colab_type": "code",
        "outputId": "88f9f8d4-7b70-43e3-8466-8301066f04b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "#import libraries\n",
        "import nltk, string\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import gensim\n",
        "from gensim.matutils import softcossim \n",
        "from gensim import corpora\n",
        "import gensim.downloader as api\n",
        "from gensim.utils import simple_preprocess\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rrycn2FASvSb",
        "colab_type": "code",
        "outputId": "079c5e3c-9eff-4481-b324-9cb2048f7bbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "#load word embedding model\n",
        "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1RbGl11S742",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loadTextFiles(a,b): #two text files were taken and they were the same\n",
        "  global doc1, doc2\n",
        "  with open(a, 'r') as file:\n",
        "      doc1 = file.read().replace('\\n', ' ')\n",
        "  with open(b, 'r') as file:\n",
        "      doc2 = file.read().replace('\\n', ' ')\n",
        "  documents = [doc1,doc2]\n",
        "  return documents"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZPRxKcNTaSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def createDocumentTermMatrix(documents):\n",
        "    # Create the Document Term Matrix\n",
        "    #CountVectorizer tokenizes text files and builds a vocabulary of known words. We passed in \"stop_words\" to exclude the common stop words such as \"a,is,the\" etc.\n",
        "    count_vectorizer = CountVectorizer(stop_words='english')\n",
        "    count_vectorizer = CountVectorizer()\n",
        "    sparse_matrix = count_vectorizer.fit_transform(documents)\n",
        "    showDocumentMatrix(sparse_matrix,count_vectorizer)\n",
        "#This function transfors our sparse matrix to a dense matrix. This means that our training data will exclude non-zero values\n",
        "#A sparse matrix has more zero values and a dense matrix has least zero values\n",
        "def showDocumentMatrix(sparse_matrix, count_vectorizer):   \n",
        "    doc_term_matrix = sparse_matrix.todense()\n",
        "    global df\n",
        "    df = pd.DataFrame(doc_term_matrix, \n",
        "                      columns=count_vectorizer.get_feature_names(), \n",
        "                      index=['doc_1', 'doc_2'])\n",
        "    print(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eltzs3kgT3On",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Get cosine similarity using Sklearn built-in function\n",
        "def getCosineSimilarity(df):\n",
        "    print(cosine_similarity(df)[0][1])\n",
        "\n",
        "#Self defined function to get cosine similarity\n",
        "def stem_tokens(tokens):\n",
        "    '''This function is used to generalise the words by removing\n",
        "    suffixes For example: If we did NOT use this function, words like \"playing,play and player will\" will be considered as different words\n",
        "    This function removes the suffixes and looks at all of those words as one word \"play\" '''\n",
        "    stemmer = nltk.stem.porter.PorterStemmer()\n",
        "    return [stemmer.stem(item) for item in tokens]\n",
        "\n",
        "'''\n",
        "In this function, we are tokenizing the texts that we have. We used text.lower() to make sure all of the words are treated the same.\n",
        "I.E - \"The\", \"thE\" and \"tHe\" are now just \"the\" \n",
        "'''\n",
        "def normalize(text):\n",
        "    remove_punctuation_map = dict((ord(char), None) for char in string.punctuation)\n",
        "    return stem_tokens(nltk.word_tokenize(text.lower()))\n",
        "\n",
        "'''\n",
        "This is where we pass our two text files to find the cosine similarity\n",
        "We use Tfidf Vectorizer transforms the texts to Vectors which we need to use in order to use our Cosine Formula.\n",
        "The cosine formula is the same as the one we use in maths and applies to vectors\n",
        "'''\n",
        "def getSelfCosineSimilarity(doc1, doc2):\n",
        "    vectorizer = TfidfVectorizer(tokenizer=normalize, stop_words='english')\n",
        "    tfidf = vectorizer.fit_transform([doc1, doc2])\n",
        "    print(((tfidf * tfidf.T).A)[0,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fiOfl6toT7Wr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Soft Cosine Similarity\n",
        "def dictionaryCorpus(documents):\n",
        "    global dictionary\n",
        "    dictionary = corpora.Dictionary([simple_preprocess(doc) for doc in documents])\n",
        "\n",
        "    \n",
        " #Here we basically are using the trained data from our model to find similarities with the data from our text files. For example:\n",
        "    #Say our trained model has the word \"boat\" in it. And our text file has the word \"ship\". This will find a pattern between these two words.\n",
        "def similarityMatrix(dictionary):\n",
        "    similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
        "    return similarity_matrix\n",
        "\n",
        "def convertToBagOfWords(dictionary,doc1,doc2):\n",
        "    sent_1 = dictionary.doc2bow(simple_preprocess(doc1))\n",
        "    sent_2 = dictionary.doc2bow(simple_preprocess(doc2))\n",
        "    global sentences\n",
        "    sentences = [sent_1, sent_2]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_Mg6iTMT_3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def softCosineSim(sent_1,sent_2,similarity_matrix):\n",
        "    print(softcossim(sent_1, sent_2, similarity_matrix))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-YoHUZgUDO_",
        "colab_type": "code",
        "outputId": "ed5b5aaa-6f9b-4ccb-ff0f-2a28b5264f24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "#put in the name of the two text files that you want to read\n",
        "documents = loadTextFiles('doc1.txt','doc2.txt')\n",
        "createDocumentTermMatrix(documents)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       60  accuracy  almost  and  assumption  average  ...  x1  x2  x3  x4  xn  zero\n",
            "doc_1   2         1       1    1           1        1  ...   4   4   2   2   1     1\n",
            "doc_2   2         1       1    1           1        1  ...   4   4   2   2   1     1\n",
            "\n",
            "[2 rows x 103 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltGC4oxxUFs3",
        "colab_type": "code",
        "outputId": "c5651a19-cac6-4b6f-b31c-4716d2045f0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "#Using cosine similarity to find similarity\n",
        "\n",
        "print(\"Using Sklearn our cosine similarity value is: \",end='')\n",
        "getCosineSimilarity(df)\n",
        "print(\"Using a self-built function with NLP tools our cosine similarity value is: \",end='')\n",
        "getSelfCosineSimilarity(doc1,doc2)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Sklearn our cosine similarity value is: 0.9999999999999987\n",
            "Using a self-built function with NLP tools our cosine similarity value is: 1.0000000000000002\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
            "  'stop_words.' % sorted(inconsistent))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVixFvUcUIzw",
        "colab_type": "code",
        "outputId": "4a69c225-16df-4ac7-a1a5-27a44c6a656c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#soft cosine similarity\n",
        "dictionaryCorpus(documents)\n",
        "similarity_matrix = similarityMatrix(dictionary)\n",
        "convertToBagOfWords(dictionary,doc1,doc2)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
            "  if np.issubdtype(vec.dtype, np.int):\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JeH8uYQUPhA",
        "colab_type": "code",
        "outputId": "6778454d-3c37-478d-d160-8140567aba26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Using Soft Cosine Similarity our value is: \",end='')\n",
        "softCosineSim(sentences[0],sentences[1],similarity_matrix)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using Soft Cosine Similarity our value is: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}